{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b23d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tatya\\OneDrive\\Documents\\auto-completion-of-texts\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from src.data_utils import clean_text\n",
    "from src.next_token_dataset import CustomDataset, make_collate_fn\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from src.eval_lstm import compute_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc64e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read txt-file\n",
    "with open('data/raw_dataset.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# delete \\n\n",
    "texts = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# clean data\n",
    "cleaned_dataset = [clean_text(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6107f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" user url awww that's a bummer you shoulda got david carr of third day to do it d\",\n",
       " \"is upset that he can't update his facebook by texting it and might cry as a result school today also blah \",\n",
       " ' user i dived many times for the ball managed to save 50 the rest go out of bounds',\n",
       " 'my whole body feels itchy and like its on fire',\n",
       " \" user no it's not behaving at all i'm mad why am i here because i can't see you all over there \"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef42785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clearned txt\n",
    "\n",
    "output_path = 'data/cleaned_data.txt'\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(cleaned_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065979b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfdb810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_id - 0, sep_id - 102, unk_id - 100, vocab_size - 30522\n"
     ]
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "pad_id = tokenizer.pad_token_id\n",
    "sep_id = tokenizer.sep_token_id\n",
    "unk_id = tokenizer.unk_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f'pad_id - {pad_id}, sep_id - {sep_id}, unk_id - {unk_id}, vocab_size - {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d839504",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict({\"text\": cleaned_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16968fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизация батчами, без добавления [CLS]/[SEP] в каждую строку\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c8ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1600498/1600498 [00:40<00:00, 39262.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1600498/1600498 [00:00<00:00, 3213324.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_tok = ds.map(\n",
    "    lambda batch: tokenizer(batch[\"text\"], add_special_tokens=False),\n",
    "    batched=True,\n",
    "    batch_size=1000,           # регулируй в зависимости от RAM\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "ds_tok.set_format(type=\"torch\")\n",
    "ds_tok.save_to_disk(\"data/tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855c2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5310, 24471,  2140, 22091,  2860,  2860,  2008,  1005,  1055,  1037,\n",
      "        26352,  5017,  2017,  2323,  2050,  2288,  2585, 12385,  1997,  2353])\n",
      "user url awww that ' s a bummer you shoulda got david carr of third\n"
     ]
    }
   ],
   "source": [
    "print(ds_tok[0][\"input_ids\"][:20])\n",
    "print(tokenizer.decode(ds_tok[0][\"input_ids\"][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0d9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ds_tok['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da01f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 1600498\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    N = len(all_ids)\n",
    "except Exception:\n",
    "    all_ids = list(all_ids)  # если маленький — ok\n",
    "    N = len(all_ids)\n",
    "\n",
    "print(\"Total sequences:\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5db88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fd72a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(N)\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.10, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.111111, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "400e49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts (indices): 1280398 160050 160050\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts (indices):\", len(train_idx), len(val_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds_tok.select(train_idx)\n",
    "val_ds   = ds_tok.select(val_idx)\n",
    "test_ds  = ds_tok.select(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ca9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1280398/1280398 [00:07<00:00, 170725.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 160050/160050 [00:00<00:00, 210315.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 160050/160050 [00:00<00:00, 169625.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# сохраняем\n",
    "train_ds.save_to_disk(\"data/train_ds\")\n",
    "val_ds.save_to_disk(\"data/val_ds\")\n",
    "test_ds.save_to_disk(\"data/test_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ceed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переводим в torch формат\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04abdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64\n",
    "STEP = 64\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c0c61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_ds)\n",
    "val_dataset   = CustomDataset(val_ds)\n",
    "test_dataset   = CustomDataset(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29964a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d998f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate = make_collate_fn(pad_id=pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c763865",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, collate_fn=collate, num_workers=0, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, collate_fn=collate, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6f35cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader is ready. Example batches: 1280398\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Loader is ready. Example batches: {len(train_ds)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3875fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lstm_model import RNN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f37950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNN(vacab_size=tokenizer.vocab_size, emb_dim=128, hidden=256, padding_idx=pad_id).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62d708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef1ba3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c96c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inps_b, tgts_b, mask_b in loader:\n",
    "        print(type(inps_b))\n",
    "        inps_b = inps_b.to(device)\n",
    "        tgts_b = tgts_b.to(device)\n",
    "        logits = model(inps_b)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inps_b)\n",
    "        loss = criterion(logits, tgts_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24928fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for inps_b, tgts_b, _ in loader:\n",
    "            inps_b = inps_b.to(device)\n",
    "            tgts_b = tgts_b.to(device)\n",
    "            logits = model(inps_b)\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            trues += tgts_b.tolist()\n",
    "    return accuracy_score(trues, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fc473d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(10):\n",
    "#     loss = train_epochs(model, train_loader)\n",
    "#     acc = evaluate(model, val_loader)\n",
    "#     print(f'Epoch {epoch+1}: Loss = {loss:.4f}, Accuracy = {acc:,4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# предполагается, что:\n",
    "# model, optimizer, criterion, device, pad_id уже определены\n",
    "# train_loader, val_loader — DataLoader, возвращающие (inps, tgts, mask)\n",
    "# inps: [B, L], tgts: [B, L], mask: [B, L] (bool) where mask = (tgts != pad_id)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0  # для усреднения по токенам (не включая паддинги)\n",
    "\n",
    "    for inps_b, tgts_b, mask_b, _ in loader:\n",
    "        # Переводим на устройство\n",
    "        inps_b = inps_b.to(device)        # [B, L]\n",
    "        tgts_b = tgts_b.to(device)        # [B, L]\n",
    "        mask_b = mask_b.to(device)        # [B, L] bool\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inps_b)            # ожидаем [B, L, V]\n",
    "        if logits.dim() != 3:\n",
    "            raise RuntimeError(f\"Expected model output [B,L,V], got {logits.shape}\")\n",
    "\n",
    "        B, L, V = logits.size()\n",
    "\n",
    "        # CrossEntropyLoss expects (N, C) and targets (N,)\n",
    "        # Складываем первые два измерения: (B*L, V), targets (B*L,)\n",
    "        logits_flat = logits.view(B * L, V)            # [B*L, V]\n",
    "        targets_flat = tgts_b.view(B * L)              # [B*L]\n",
    "\n",
    "        loss = criterion(logits_flat, targets_flat)    # ignore_index=pad_id учтёт паддинг\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Считаем метрики: суммируем loss (scale по батчам)\n",
    "        # Для корректного усреднения берём количество ненулевых токенов (mask)\n",
    "        n_tokens = mask_b.sum().item()\n",
    "        total_loss += loss.item() * n_tokens           # loss суммирован по токенам\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('nan')\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inps_b, tgts_b, mask_b, _ in loader:\n",
    "            inps_b = inps_b.to(device)\n",
    "            tgts_b = tgts_b.to(device)\n",
    "            mask_b = mask_b.to(device)\n",
    "\n",
    "            logits = model(inps_b)               # [B, L, V]\n",
    "            B, L, V = logits.size()\n",
    "\n",
    "            logits_flat = logits.view(B * L, V)\n",
    "            targets_flat = tgts_b.view(B * L)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            n_tokens = mask_b.sum().item()\n",
    "            total_loss += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "\n",
    "            # Предсказания: argmax по измерению словаря -> shape [B, L]\n",
    "            preds = torch.argmax(logits, dim=2).cpu()   # [B, L]\n",
    "            trues = tgts_b.cpu()                        # [B, L]\n",
    "\n",
    "            # Добавляем только непаддинговые позиции в списки для accuracy\n",
    "            mask_cpu = mask_b.cpu()\n",
    "            # флаттеним и фильтруем\n",
    "            preds_flat = preds.view(-1)\n",
    "            trues_flat = trues.view(-1)\n",
    "            mask_flat = mask_cpu.view(-1)\n",
    "            if mask_flat.sum().item() > 0:\n",
    "                all_preds.extend(preds_flat[mask_flat].tolist())\n",
    "                all_trues.extend(trues_flat[mask_flat].tolist())\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('nan')\n",
    "    accuracy = accuracy_score(all_trues, all_preds) if len(all_trues) > 0 else float('nan')\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Пример использования в цикле обучения\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    rouge_scores = compute_rouge(model, val_loader, tokenizer, device, pad_id)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  TrainLoss={train_loss:.6f}  ValLoss={val_loss:.6f} \"\n",
    "          f\"ROUGE-1={rouge_scores['rouge1']:.4f}  ROUGE-2={rouge_scores['rouge2']:.4f}  ROUGE-L={rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}/{num_epochs}  TrainLoss={train_loss:.6f}  ValLoss={val_loss:.6f}  ValAcc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4930adca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33mmodels/rnn_lm.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Сохраняем веса\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtorch\u001b[49m.save(model.state_dict(), model_path)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Для загрузки\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 1. создаём объект модели с такой же архитектурой\u001b[39;00m\n\u001b[32m      9\u001b[39m loaded_model = RNN(vacab_size=tokenizer.vocab_size, emb_dim=\u001b[32m128\u001b[39m, hidden=\u001b[32m256\u001b[39m, padding_idx=pad_id).to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Путь для сохранения\n",
    "model_path = \"models/rnn_lm.pt\"\n",
    "\n",
    "# Сохраняем веса\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Для загрузки\n",
    "# 1. создаём объект модели с такой же архитектурой\n",
    "loaded_model = RNN(vacab_size=tokenizer.vocab_size, emb_dim=128, hidden=256, padding_idx=pad_id).to(device)\n",
    "\n",
    "# 2. загружаем веса\n",
    "loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "loaded_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e13a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user facebook iphone app is\n",
      "user facebook iphone app is working 3 years ago still surviving ink you ' re making me see the companies more effort to even need 4 a life jealous not hear that u still pick\n"
     ]
    }
   ],
   "source": [
    "seed = train_ds[0][\"input_ids\"][:5]  \n",
    "\n",
    "# генерируем 30 токенов\n",
    "gen_tokens = loaded_model.generate(seed, max_len=30, temperature=1.0, pad_id=pad_id, device=device)\n",
    "\n",
    "# обратно в текст\n",
    "generated_text = tokenizer.decode(gen_tokens)\n",
    "print(tokenizer.decode(seed))\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa292e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptuna\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hidden = trial.suggest_categorical('hidden', [128, 256])\n",
    "    emb = trial.suggest_categorical('emb', [64, 128])\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    bs = trial.suggest_categorical('batch_size', [64, 128])\n",
    "\n",
    "    model = RNN(vacab_size=tokenizer.vocab_size, emb_dim=emb, hidden=hidden, padding_idx=pad_id).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=bs, collate_fn=collate_fn)\n",
    "\n",
    "    # тренируем 2-3 эпохи для оценки\n",
    "    for epoch in range(2):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    val_loss, _ = evaluate(model, val_loader, criterion, device)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # 20 экспериментов\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
