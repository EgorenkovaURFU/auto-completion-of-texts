{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce507f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.data_utils import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e77085",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfc0cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read txt-file\n",
    "with open('data/raw_dataset.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# delete \\n\n",
    "texts = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# clean data\n",
    "cleaned_dataset = [clean_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa125859",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(cleaned_dataset)\n",
    "train_size = int(0.8 * n)\n",
    "val_size = int(0.1 * n)\n",
    "\n",
    "train_texts = cleaned_dataset[:train_size]\n",
    "val_texts = cleaned_dataset[train_size:train_size + val_size]\n",
    "test_texts = cleaned_dataset[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd32666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# GPT2 не имеет padding token по умолчанию, добавим\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "242aa99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        \"\"\"\n",
    "        texts: list of items, где item может быть:\n",
    "            - str  (сырой текст)\n",
    "            - list[int] (токены)\n",
    "            - torch.Tensor (1D tensor of token ids)\n",
    "        tokenizer: transformers tokenizer (used if item is str)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def _convert_item_to_ids(self, item):\n",
    "        # 1) если это уже torch.Tensor -> вернуть long tensor\n",
    "        if isinstance(item, torch.Tensor):\n",
    "            return item.long()\n",
    "        # 2) если это список/кортеж\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            # если элементы списка — числа -> считаем это list of ids\n",
    "            if len(item) == 0:\n",
    "                return torch.tensor([], dtype=torch.long)\n",
    "            first = item[0]\n",
    "            # если список целых — преобразуем напрямую\n",
    "            if isinstance(first, (int,)):\n",
    "                return torch.tensor(list(item), dtype=torch.long)\n",
    "            # если список строк (например, список слов/символов) — соберём в строку и токенизируем\n",
    "            if isinstance(first, str):\n",
    "                # небольшая эвристика: если элементы — одиночные символы (len==1), склеим без пробелов\n",
    "                if all(isinstance(t, str) and len(t) == 1 for t in item):\n",
    "                    text = \"\".join(item)\n",
    "                else:\n",
    "                    # иначе предполагаем список слов -> соединяем пробелом\n",
    "                    text = \" \".join(item)\n",
    "                enc = self.tokenizer.encode(text, truncation=True, max_length=self.max_len)\n",
    "                return torch.tensor(enc, dtype=torch.long)\n",
    "            # иначе — попробуем сконвертировать элементарно\n",
    "            return torch.tensor(list(item), dtype=torch.long)\n",
    "\n",
    "        # 3) если это строка — токенизируем\n",
    "        if isinstance(item, str):\n",
    "            enc = self.tokenizer.encode(item, truncation=True, max_length=self.max_len)\n",
    "            return torch.tensor(enc, dtype=torch.long)\n",
    "\n",
    "        # 4) если ни одно из выше — бросаем ошибку\n",
    "        raise TypeError(f\"Unsupported item type: {type(item)}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # поддержка слайса: возвращаем список элементов в том же формате, как обычный Python slicing\n",
    "        if isinstance(idx, slice):\n",
    "            indices = range(*idx.indices(len(self)))\n",
    "            return [self._convert_item_to_ids(self.texts[i]) for i in indices]\n",
    "\n",
    "        # single index\n",
    "        item = self.texts[idx]\n",
    "        ids = self._convert_item_to_ids(item)\n",
    "        return ids\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5dd7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(model, tokenizer, input_ids, gen_fraction=0.25, max_len=None):\n",
    "    \"\"\"\n",
    "    input_ids: torch.Tensor [L]\n",
    "    \"\"\"\n",
    "    L = input_ids.size(0)\n",
    "    L_gen = int(L * gen_fraction)\n",
    "    seed = input_ids[:L - L_gen].unsqueeze(0).to(device)  # batch 1\n",
    "\n",
    "    gen_output = model.generate(\n",
    "        seed,\n",
    "        max_length=(seed.size(1) + L_gen),\n",
    "        do_sample=False,       # greedy\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    return gen_output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35893051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def compute_rouge_gpt(model, dataset, tokenizer, gen_fraction=0.25):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "    scores_list = []\n",
    "\n",
    "    for input_ids in dataset:\n",
    "        input_ids = input_ids.to(device)\n",
    "        L = input_ids.size(0)\n",
    "        L_gen = int(L * gen_fraction)\n",
    "\n",
    "        gen_ids = generate_completion(model, tokenizer, input_ids, gen_fraction=gen_fraction)\n",
    "        gen_text = tokenizer.decode(gen_ids[L - L_gen:], skip_special_tokens=True)\n",
    "        tgt_text = tokenizer.decode(input_ids[L - L_gen:], skip_special_tokens=True)\n",
    "\n",
    "        scores = scorer.score(tgt_text, gen_text)\n",
    "        scores_list.append(scores)\n",
    "\n",
    "    # усреднение\n",
    "    avg_scores = {}\n",
    "    for key in ['rouge1','rouge2','rougeL']:\n",
    "        avg_scores[key] = sum([s[key].fmeasure for s in scores_list]) / len(scores_list)\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3366a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  quot girl with beautiful eyes quot joined as user 's colleague in ascendas chennai\n",
      "Generated:  he wants the name help him ㅎㅎㅎ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "Input: peeing a river ahh my best game\n",
      "Generated:  of beer pong\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "Input:  user she is so fucking adorable aww\n",
      "Generated:  i love kittens.\n",
      "I love you.\n",
      "---\n",
      "Input:  user love that\n",
      "Generated:  thing, and I really love it. And I've heard that the thing is you're just gonna want to do it to do it, because if you do it then you'll want to do it because it's a fantastic idea. So the problem is\n",
      "---\n",
      "Input: good morning from\n",
      "Generated:  italy,” he said. …It’s good to have the good people and the bad people.”\n",
      "\n",
      "\n",
      "…There’s a whole other story about the night,” he said. …We\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(train_texts, tokenizer)\n",
    "val_dataset   = TextDataset(val_texts, tokenizer)\n",
    "\n",
    "# безопасный перебор первых 5 примеров:\n",
    "for i in range(5):\n",
    "    input_ids = val_dataset[i].unsqueeze(0).to(device)  # shape [1, L]\n",
    "    gen_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    L = input_ids.size(1)\n",
    "    seed_len = int(0.75 * L)\n",
    "    print(\"Input:\", tokenizer.decode(input_ids[0, :seed_len].cpu().tolist(), skip_special_tokens=True))\n",
    "    print(\"Generated:\", tokenizer.decode(gen_ids[0, seed_len:].cpu().tolist(), skip_special_tokens=True))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "512c3c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1=0.0224  ROUGE-2=0.0022  ROUGE-L=0.0224\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1️⃣ Настройка модели и устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# 2️⃣ Подготовка данных (используем твой класс TextDataset)\n",
    "val_dataset = TextDataset(val_texts, tokenizer, max_len=128)\n",
    "\n",
    "# 3️⃣ Функция генерации (используем простую версию для GPT)\n",
    "def generate_completion(model, tokenizer, input_ids, gen_fraction=0.25, max_new_tokens=50):\n",
    "    \"\"\"Генерация продолжения текста на основе части входа\"\"\"\n",
    "    model.eval()\n",
    "    input_ids = input_ids.unsqueeze(0).to(device)\n",
    "    L = input_ids.size(1)\n",
    "    seed_len = int(L * (1 - gen_fraction))\n",
    "\n",
    "    gen_ids = model.generate(\n",
    "        input_ids[:, :seed_len],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return gen_ids[0].cpu()\n",
    "\n",
    "# 4️⃣ Вставляем твою функцию compute_rouge_gpt\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def compute_rouge_gpt(model, dataset, tokenizer, gen_fraction=0.25):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "    scores_list = []\n",
    "\n",
    "    for input_ids in dataset[:20]:  # ограничим 20 примерами для скорости\n",
    "        input_ids = input_ids.to(device)\n",
    "        L = input_ids.size(0)\n",
    "        L_gen = int(L * gen_fraction)\n",
    "\n",
    "        gen_ids = generate_completion(model, tokenizer, input_ids, gen_fraction=gen_fraction)\n",
    "        gen_text = tokenizer.decode(gen_ids[L - L_gen:], skip_special_tokens=True)\n",
    "        tgt_text = tokenizer.decode(input_ids[L - L_gen:], skip_special_tokens=True)\n",
    "\n",
    "        scores = scorer.score(tgt_text, gen_text)\n",
    "        scores_list.append(scores)\n",
    "\n",
    "    # усреднение\n",
    "    avg_scores = {}\n",
    "    for key in ['rouge1','rouge2','rougeL']:\n",
    "        avg_scores[key] = sum([s[key].fmeasure for s in scores_list]) / len(scores_list)\n",
    "    return avg_scores\n",
    "\n",
    "# 5️⃣ Запуск вычисления метрик\n",
    "rouge_scores = compute_rouge_gpt(model, val_dataset, tokenizer, gen_fraction=0.25)\n",
    "print(f\"ROUGE-1={rouge_scores['rouge1']:.4f}  ROUGE-2={rouge_scores['rouge2']:.4f}  ROUGE-L={rouge_scores['rougeL']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
